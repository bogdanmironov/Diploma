{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "train_file = 'MA0035_4_m5_train.h5'\n",
    "test_file = 'MA0035_4_m5_test.h5'\n",
    "\n",
    "h5_train = h5py.File(train_file, 'r')\n",
    "h5_test = h5py.File(test_file, 'r')\n",
    "\n",
    "train_data = h5_train['data'][:90000]\n",
    "train_binlabels = h5_train['binlabels'][:90000]\n",
    "\n",
    "val_data = h5_train['data'][-10000:]\n",
    "val_binlabels = h5_train['binlabels'][-10000:]\n",
    "\n",
    "test_data = h5_test['data'][:]\n",
    "test_binlabels = h5_test['binlabels'][:]\n",
    "\n",
    "train_data = tf.cast(train_data, dtype=tf.float32)\n",
    "train_labels = tf.cast(train_binlabels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(90000, 4, 1000)\n",
      "(90000, 1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "train_data_T = np.transpose(train_data, axes=(0, 2, 1))\n",
    "print(train_data_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 4, 1000)\n(10000, 1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "test_data_T = np.transpose(test_data, axes=(0, 2, 1))\n",
    "print(test_data_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 4, 1000)\n(10000, 1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(val_data.shape)\n",
    "val_data_T = np.transpose(val_data, axes=(0, 2, 1))\n",
    "print(val_data_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(hyperparameters, log_dir):\n",
    "    board_log = log_dir + '/board'\n",
    "    hparams_log = log_dir + '/hparams.json'\n",
    "\n",
    "    my_callbacks = [\n",
    "        TensorBoard(log_dir=board_log),\n",
    "        EarlyStopping('val_loss', patience=100)\n",
    "    ]\n",
    "\n",
    "    with open(hparams_log, 'w') as hparam_file:\n",
    "        json.dump(hyperparameters, hparam_file)\n",
    "\n",
    "    # model = tf.keras.models.Sequential(\n",
    "    #     [\n",
    "    #         tf.keras.layers.Flatten(input_shape=(4, 1000)),\n",
    "    #         tf.keras.layers.Dropout(hyperparameters['l0_dropout_rate']),\n",
    "    #         tf.keras.layers.Dense(hyperparameters['l1_hidden_units'], activation=tf.keras.activations.relu),\n",
    "    #         # tf.keras.layers.Dense(hyperparameters['l2_hidden_units'], activation=tf.keras.activations.relu),\n",
    "    #         # tf.keras.layers.Dropout(hyperparameters['l2_dropout_rate']),\n",
    "    #         tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "    #     ]\n",
    "    # )\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            Flatten(input_shape=(1000, 4)),\n",
    "            Dropout(hyperparameters['l0_dropout_rate']),\n",
    "            Dense(100, activation='relu', kernel_regularizer='l2', bias_regularizer='l2'),\n",
    "            Dropout(0.5),\n",
    "            Dense(30, activation='relu'),\n",
    "            Dropout(0.6),\n",
    "            Dense(30, activation='relu', kernel_regularizer='l2', bias_regularizer='l2'),\n",
    "            Dense(5, activation='relu'),\n",
    "            Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "        model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "            loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=[tf.keras.metrics.AUC(name='auc')],\n",
    "        )\n",
    "\n",
    "        history = model.fit(train_data_T, train_binlabels,\n",
    "                epochs=hyperparameters['epochs'], \n",
    "                validation_data=(val_data_T, val_binlabels),\n",
    "                batch_size=hyperparameters['batch_size'],\n",
    "                callbacks=my_callbacks,\n",
    "                shuffle=True)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_roc(false_positive_rate, true_positive_rate, log_dir):\n",
    "    roc = plt.figure(0)\n",
    "\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(log_dir + '/roc.pdf')\n",
    "    pickle.dump(roc, open((log_dir + '/pickle/roc.fig.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_loss(history, log_dir):\n",
    "    loss = plt.figure(1)\n",
    "    history_loss = history.history['loss']\n",
    "    history_val_loss = history.history['val_loss']\n",
    "    epochs = range(len(history_loss))\n",
    "\n",
    "    plt.plot(epochs, history_loss, 'ko', label='Training loss')\n",
    "    plt.plot(epochs, history_val_loss, 'b', label='Validation loss')\n",
    "    plt.ylim([0.0, 1.5])\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(log_dir + '/loss.pdf')\n",
    "    pickle.dump(loss, open((log_dir + '/pickle/loss.fig.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_auc(history, log_dir):\n",
    "    auc = plt.figure(2)\n",
    "\n",
    "    history_auc = history.history['auc']\n",
    "    history_val_auc = history.history['val_auc']\n",
    "    epochs = range(len(history_auc))\n",
    "\n",
    "    plt.plot(epochs, history_auc, 'ko', label='Training auc')\n",
    "    plt.plot(epochs, history_val_auc, 'b', label='Validation auc')\n",
    "    plt.title('Training and validation auc')\n",
    "    plt.ylim(0.46, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(log_dir + '/auc.pdf')\n",
    "    pickle.dump(auc, open((log_dir + '/pickle/auc.fig.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n",
      "704/704 [==============================] - 3s 3ms/step - loss: 2.8957 - auc: 0.4992 - val_loss: 2.8548 - val_auc: 0.5050\n",
      "Epoch 2/300\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 2.8308 - auc: 0.4962 - val_loss: 2.7948 - val_auc: 0.5024\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.7670 - auc: 0.5041 - val_loss: 2.7353 - val_auc: 0.5024\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 2.7099 - auc: 0.5006 - val_loss: 2.6765 - val_auc: 0.5008\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 2.6492 - auc: 0.5034 - val_loss: 2.6172 - val_auc: 0.4994\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.5893 - auc: 0.5027 - val_loss: 2.5572 - val_auc: 0.4976\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.5274 - auc: 0.4986 - val_loss: 2.4980 - val_auc: 0.4984\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.4697 - auc: 0.5054 - val_loss: 2.4396 - val_auc: 0.4977\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.4099 - auc: 0.5011 - val_loss: 2.3807 - val_auc: 0.4971\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.3512 - auc: 0.4974 - val_loss: 2.3231 - val_auc: 0.4970\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.2932 - auc: 0.5030 - val_loss: 2.2664 - val_auc: 0.4961\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.2376 - auc: 0.5055 - val_loss: 2.2100 - val_auc: 0.4973\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.1818 - auc: 0.4984 - val_loss: 2.1547 - val_auc: 0.4960\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.1257 - auc: 0.5034 - val_loss: 2.1010 - val_auc: 0.4959\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.0727 - auc: 0.5070 - val_loss: 2.0489 - val_auc: 0.4980\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.0219 - auc: 0.5027 - val_loss: 1.9987 - val_auc: 0.4971\n",
      "Epoch 17/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.9699 - auc: 0.5049 - val_loss: 1.9492 - val_auc: 0.4968\n",
      "Epoch 18/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.9220 - auc: 0.5066 - val_loss: 1.9010 - val_auc: 0.4971\n",
      "Epoch 19/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.8746 - auc: 0.5058 - val_loss: 1.8549 - val_auc: 0.5004\n",
      "Epoch 20/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.8276 - auc: 0.5060 - val_loss: 1.8109 - val_auc: 0.4981\n",
      "Epoch 21/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.7835 - auc: 0.5061 - val_loss: 1.7682 - val_auc: 0.5013\n",
      "Epoch 22/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.7436 - auc: 0.5020 - val_loss: 1.7271 - val_auc: 0.5016\n",
      "Epoch 23/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.7019 - auc: 0.5068 - val_loss: 1.6865 - val_auc: 0.5015\n",
      "Epoch 24/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.6616 - auc: 0.5058 - val_loss: 1.6479 - val_auc: 0.5024\n",
      "Epoch 25/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.6213 - auc: 0.5108 - val_loss: 1.6104 - val_auc: 0.5026\n",
      "Epoch 26/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.5858 - auc: 0.5015 - val_loss: 1.5754 - val_auc: 0.5016\n",
      "Epoch 27/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.5508 - auc: 0.5082 - val_loss: 1.5413 - val_auc: 0.5016\n",
      "Epoch 28/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.5171 - auc: 0.5071 - val_loss: 1.5075 - val_auc: 0.5011\n",
      "Epoch 29/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.4859 - auc: 0.5033 - val_loss: 1.4758 - val_auc: 0.5017\n",
      "Epoch 30/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.4521 - auc: 0.5102 - val_loss: 1.4452 - val_auc: 0.5009\n",
      "Epoch 31/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.4236 - auc: 0.5053 - val_loss: 1.4167 - val_auc: 0.5006\n",
      "Epoch 32/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.3949 - auc: 0.5090 - val_loss: 1.3893 - val_auc: 0.5016\n",
      "Epoch 33/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.3665 - auc: 0.5069 - val_loss: 1.3623 - val_auc: 0.5036\n",
      "Epoch 34/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.3388 - auc: 0.5150 - val_loss: 1.3372 - val_auc: 0.5037\n",
      "Epoch 35/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.3153 - auc: 0.5072 - val_loss: 1.3121 - val_auc: 0.5023\n",
      "Epoch 36/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.2905 - auc: 0.5061 - val_loss: 1.2891 - val_auc: 0.5023\n",
      "Epoch 37/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.2687 - auc: 0.5014 - val_loss: 1.2665 - val_auc: 0.5025\n",
      "Epoch 38/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.2491 - auc: 0.5066 - val_loss: 1.2445 - val_auc: 0.5045\n",
      "Epoch 39/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.2271 - auc: 0.5064 - val_loss: 1.2238 - val_auc: 0.5059\n",
      "Epoch 40/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.2044 - auc: 0.5046 - val_loss: 1.2049 - val_auc: 0.5037\n",
      "Epoch 41/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1863 - auc: 0.5116 - val_loss: 1.1854 - val_auc: 0.5030\n",
      "Epoch 42/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1653 - auc: 0.5129 - val_loss: 1.1677 - val_auc: 0.5038\n",
      "Epoch 43/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1507 - auc: 0.5131 - val_loss: 1.1503 - val_auc: 0.5024\n",
      "Epoch 44/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1327 - auc: 0.5097 - val_loss: 1.1332 - val_auc: 0.5027\n",
      "Epoch 45/300\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 1.1177 - auc: 0.5070 - val_loss: 1.1181 - val_auc: 0.5031\n",
      "Epoch 46/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0984 - auc: 0.5132 - val_loss: 1.1033 - val_auc: 0.5036\n",
      "Epoch 47/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0839 - auc: 0.5152 - val_loss: 1.0881 - val_auc: 0.5019\n",
      "Epoch 48/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0721 - auc: 0.5120 - val_loss: 1.0732 - val_auc: 0.5056\n",
      "Epoch 49/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0556 - auc: 0.5150 - val_loss: 1.0606 - val_auc: 0.5040\n",
      "Epoch 50/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0441 - auc: 0.5108 - val_loss: 1.0478 - val_auc: 0.5030\n",
      "Epoch 51/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0325 - auc: 0.5141 - val_loss: 1.0352 - val_auc: 0.5025\n",
      "Epoch 52/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0188 - auc: 0.5177 - val_loss: 1.0228 - val_auc: 0.5046\n",
      "Epoch 53/300\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 1.0061 - auc: 0.5164 - val_loss: 1.0117 - val_auc: 0.5046\n",
      "Epoch 54/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9946 - auc: 0.5185 - val_loss: 1.0004 - val_auc: 0.5032\n",
      "Epoch 55/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9851 - auc: 0.5171 - val_loss: 0.9899 - val_auc: 0.5019\n",
      "Epoch 56/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9760 - auc: 0.5158 - val_loss: 0.9790 - val_auc: 0.5051\n",
      "Epoch 57/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9660 - auc: 0.5171 - val_loss: 0.9701 - val_auc: 0.5042\n",
      "Epoch 58/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9591 - auc: 0.5163 - val_loss: 0.9593 - val_auc: 0.5060\n",
      "Epoch 59/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9463 - auc: 0.5216 - val_loss: 0.9505 - val_auc: 0.5039\n",
      "Epoch 60/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9366 - auc: 0.5187 - val_loss: 0.9421 - val_auc: 0.5041\n",
      "Epoch 61/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9316 - auc: 0.5121 - val_loss: 0.9330 - val_auc: 0.5040\n",
      "Epoch 62/300\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.9194 - auc: 0.5197 - val_loss: 0.9251 - val_auc: 0.5047\n",
      "Epoch 63/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9114 - auc: 0.5202 - val_loss: 0.9169 - val_auc: 0.5052\n",
      "Epoch 64/300\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.9045 - auc: 0.5140 - val_loss: 0.9092 - val_auc: 0.5045\n",
      "Epoch 65/300\n",
      "473/704 [===================>..........] - ETA: 0s - loss: 0.8986 - auc: 0.5242"
     ]
    }
   ],
   "source": [
    "L0_DROPOUT_RATES = [0.0, 0.5, 0.52, 0.56, 0.58, 0.6, 0.62, 0.64, 0.66, 0.68]\n",
    "# L0_DROPOUT_RATES = [0.7]\n",
    "# L1_DROPOUT_RATES = [0.0]\n",
    "# L2_DROPOUT_RATES = [0.0]\n",
    "# L1_HIDDEN_UNITS = [16]\n",
    "# L2_HIDDEN_UNITS = [128]\n",
    "BATCH_SIZE = [32, 64, 128, 254, 512]\n",
    "# LEARNING_RATE = [0.000009]\n",
    "LEARNING_RATE = [0.0000023]\n",
    "\n",
    "# for l1_hidden_units in L1_HIDDEN_UNITS:\n",
    "#     for learning_rate in LEARNING_RATE:\n",
    "for l0_dropout_rate in L0_DROPOUT_RATES:\n",
    "#             for l1_dropout_rate in L1_DROPOUT_RATES:\n",
    "    log_dir = './logs/4_h_layer_test/:' + str(l0_dropout_rate) + '|hu100l2|dr0.5|hu30|dr0.6|hu30l2|hu5lr' + str(LEARNING_RATE[0])\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(log_dir)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(log_dir)\n",
    "        os.makedirs(log_dir + '/pickle')\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "\n",
    "    hparams = {\n",
    "        'l0_dropout_rate': l0_dropout_rate,\n",
    "        'learning_rate': LEARNING_RATE[0],\n",
    "        'batch_size': BATCH_SIZE[2],\n",
    "        'epochs': 300,\n",
    "    }\n",
    "\n",
    "    model, history = train_model(hparams, log_dir)\n",
    "\n",
    "    acc, auc = model.evaluate(test_data_T, test_binlabels)\n",
    "    yhat = model.predict(test_data)\n",
    "    fpr, tpr, _ = roc_curve(test_binlabels, yhat)\n",
    "    roc_auc = roc_auc_score(test_binlabels, yhat)\n",
    "\n",
    "    plot_and_save_roc(fpr, tpr, log_dir)\n",
    "    plot_and_save_loss(history, log_dir)\n",
    "    plot_and_save_auc(history, log_dir)\n",
    "\n",
    "    plt.show()\n",
    "    model.save(log_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit27940071741648109a666d4d70900ef3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}